---
title: "Seoul Bike Sharing"
author:
    - Brocco Mattia, 2044714
    - Magliani Jacopo, 2040912
output:
  html_document:
    css: style.css
  pdf_document: default
  df_print: paged
---


## 1. INTRODUCTION

The study hereby presented concerns the topic of demand forecasting, and the case study at hand entails the industry of bike sharing for the city during a time span of one year (between 2017 and 2018) in Seoul (South Korea).
Data is available at https://archive.ics.uci.edu/ml/index.php.

This project aims at assessing to which extent it is possible to predict the demand for bikes given the data at hand in order to, ultimately, provide some additional insights on the public sharing industry.
For the sake of clearness, the target variable will be called "Rented.Bike.Count", and it is inherently a non-negative continuous variable.

List of packages used

* car
* leaps
* Hmisc
* glmnet
* mosaic
* ggplot2
* corrplot
* data.table
* fastDummies

```{r, include = FALSE}
library(car)
library(leaps)
library(Hmisc)
library(glmnet)
library(mosaic)
library(ggplot2)
library(corrplot)
library(data.table)
library(fastDummies)
```



```{r, echo = FALSE}
#We found some problems with the make.names(...) function. thus we removed the '°' symbol from temperature in the csv file.
df <- read.csv("SeoulBikeData.csv", header = TRUE)
str(df)
```

The multivariate dataframe consists of 8760 observations and 14 variables. Since for each day we have access to the data for each hour, this means we have 365 distinct days.

```{r, echo = FALSE}
print(df[c(1,dim(df)[1]),"Date"],max.levels = 0)
```

## 2. EXPLORATORY DATA ANALYSIS

In the first place we want to assess the "health" of the data.

1) NA values
```{r}
# no empty values are present
sum(any(is.na(df[,])))
```
2) Assessment on time windows and "functioning" flag.


There are 18 holiday days in the dataset (New Years Eve, Buddha's birthday, Day of Independence and so on)
```{r,echo=FALSE}
print(c(unique(df[df[,"Holiday"]=="Holiday",]["Date"])), max.levels = 0)
```

While it is useful to have separated data for each hour, we won't be able to understand whether the target variable is influenced by a specific holiday (e.g. New Year's Eve) given that we only have data for one year. What we will show, however, is whether the fact that one day is holiday or not has an impact on the target variable.


The feature "Functioning.day" indicates if there is data about the number of bikes rented. We can see that for 12 days there are no data for the entire day, while in 06/10/2018 there are data from 7 am onwards.
```{r,echo=FALSE}
table(df$Functioning.Day)
print(c(unique(df[df[,"Functioning.Day"] == "No",]["Date"])), max.levels = 0)
print(c(df[df$Date == "06/10/2018",][, c("Hour", "Functioning.Day")]), max.levels = 0)
```

Next, some tables are proposed in order to better understand how categorical regressors are distributed throughtout the dataset. 
```{r}
par(mfrow = c(1, 3))
barchart(df$Seasons, horizontal = F)
barchart(df$Holiday, horizontal = F)
barchart(df$Functioning.Day, horizontal = F)
```




### 2.1. Graphical Data exploration

In the first place, regressors are taken into consideration for (graphical) exploratory analysis.


Through a pivot table, we can assess more clearly the behavior of the independent variables. This because for most of the weather conditions, the information recoreded at each hour of the day may result messy at this point of the analysis.
Therefore, the following object is meant to group variables on each date, based on a function (that can be either max or mean).
```{r}
# This data type conversion will ease graphical representations
df$Date <- as.Date(df$Date, format = "%d/%m/%Y")


# Create a new data frame to store aggregates of variables by date
pivot.table <- aggregate(Rented.Bike.Count~Date, df, FUN = mean)

pivot.table$Max.Day.Solar.Radiation <- aggregate(Solar.Radiation..MJ.m2.~Date,
                                                df, FUN = max)$Solar.Radiation..MJ.m2.
pivot.table$Max.Day.Wind.Speed <- aggregate(Wind.speed..m.s.~Date,
                                                df, FUN = max)$Wind.speed..m.s.
pivot.table$Max.Day.Rainfall <- aggregate(Rainfall.mm.~Date,
                                                df, FUN = max)$Rainfall.mm.
pivot.table$Max.Day.Snowfall <- aggregate(Snowfall..cm.~Date,
                                                df, FUN = max)$Snowfall..cm.
pivot.table$Avg.Day.Humidity <- aggregate(Humidity...~Date,
                                          df, FUN = mean)$Humidity...
pivot.table$Avg.Day.Visibility <- aggregate(Visibility..10m.~Date,
                                            df, FUN = mean)$Visibility..10m.
```



```{r}
##############################
# Weather and time of the year
##############################

par(mfrow = c(3, 3))

plot(df$Date, df$Temperature..C., xlab = "Date", type = "l",
     ylab = "Temperature", main = "Temperature (°C) over the year")
plot(df$Date, df$Dew.point.temperature..C., xlab = "Date", type = "l",
     ylab = "Dew Point temp.", main = "Dew Point Temp. (°C) over the year")
# data from pivot.table (solar radiation has MAX function)
plot(pivot.table$Date, pivot.table$Max.Day.Solar.Radiation, xlab = "Date",
     ylab = "Solar Radiation", main = "Daily max Solar radiation (MJ/m2)", type = "l")
# data from pivot.table (humidity has AVG function)
plot(pivot.table$Date, pivot.table$Avg.Day.Humidity, xlab = "Date", ylab = "Humidity",
     main = "Avg daily Humidity (%) over the year", type = "l")
# data from pivot.table (visibility has AVG function)
plot(pivot.table$Date, pivot.table$Avg.Day.Visibility, xlab = "Date",
     ylab = "Avg daily Visibility 10m", main = "Visibility (m) over the year", type = "l")
# data from pivot.table (wind speed has MAX function)
plot(pivot.table$Date, pivot.table$Max.Day.Wind.Speed, xlab = "Date",
     ylab = "Wind speed", main = "Max daily Wind speed (m/s) over the year", type = "l")
# data from pivot.table (rainfall has MAX function)
plot(pivot.table$Date, pivot.table$Max.Day.Rainfall, xlab = "Date",
     ylab = "Rainfall", main = "Max daily Rainfall (mm) over the year", type = "l")
# data from pivot.table (snowfall has MAX function)
plot(pivot.table$Date, pivot.table$Max.Day.Snowfall, xlab = "Date",
     ylab = "Snowfall", main = "Max daily Snowfall (cm) over the year", type = "l")
```
```{r}
# Compare more than one regressor at a time
densityplot(~Temperature..C., data = df, groups = Seasons,
            auto.key = TRUE, main = "Tempereature by season")

densityplot(~Humidity..., data = df, groups = Seasons,
            auto.key = TRUE, main = "Humidity by season")

densityplot(~Rainfall.mm., data = df, groups = Seasons, xlim = c(-1, 5),
            auto.key = TRUE, main = "Rainfall by season")

densityplot(~Visibility..10m., data = df, groups = Seasons,
            auto.key = TRUE, main = "Visibility by season")
```



Next, we are interested in the independent (or target) variable of the study. The following representations aim at assessing the behavior of the latter, as well as the extent of generic relationships between the target variable and the available regressors.

```{r}
hist(df$Rented.Bike.Count, main = "Rented Bikes count")
boxplot(df$Rented.Bike.Count, main = "Rented Bikes count", horizontal = T)
```
From this first plot we see how the target variable distribution is left-skewed and pretty far from a normal distribution (consdering also the sample size).


```{r, echo = FALSE}
#add feature to have the total bikes rented in each day
plot(pivot.table$Date, pivot.table$Rented.Bike.Count,
     main = "Bikes rented for day", xlab = "Day", ylab = "Rented Bikes")
```

We can see the average number of bikes rented for day is low in Winter, increases in Spring, reaches the maximum in June, slightly decreases from June to August (probably some users go outside of the city during the summer break that goes from 22/06 to 31/08), increases again in September (end of the summer break) and then starts decreasing in October.

```{r, echo = FALSE}
# Convert "Seasons" as factor and set a precise ordering
df$Seasons <- factor(df$Seasons, levels = c("Spring", "Summer", "Autumn", "Winter"))
plot(df$Seasons, df$Rented.Bike.Count, main = "Bikes rented for season",
     xlab = "Season", ylab = "Rented Bikes")
```

We can see the average number of bikes rented in Winter is much less than the average number in the other seasons (because of the low temperatures). 
Summer has the highest average, while Autumn has a higher average than Spring probably because it benefits from the hot climate still present for a few weeks at the end of the summer season.



Non-Holiday days are characterized by an average of rented bikes greater than that of Holidays days. This is due to the small number of Holiday days in the year.

```{r,echo=FALSE}
df$Holiday <- as.factor(df$Holiday)
plot(df$Holiday, df$Rented.Bike.Count, main = "Bikes rented for holiday",
     xlab = "Holiday", ylab = "Rented Bikes")
```

We add a factor variable related to the hour time.

```{r}
df$Time <- ifelse(df$Hour >= 18, "Evening",
                  ifelse(df$Hour >= 12, "Afternoon",
                         ifelse(df$Hour >= 6, "Morning",
                                ifelse(df$Hour >= 0, "Night"))))
df$Time <- factor(df$Time, levels = c("Morning", "Afternoon", "Evening", "Night"))

plot(df$Time, df$Rented.Bike.Count, main = "Bikes rented for Time",
     xlab = "Time", ylab = "Rented Bikes")
```

## 3. FEATURE ENGINEERING


As shown above, "functioning day" states whether data for that specific time stamp (day, hour) is available or not. Therefore, we discard the examples that have "NO". We then drop the column because it becomes meaningless.
```{r}
XY <- df[df[,"Functioning.Day"] == "Yes",]
XY <- subset(XY, select = -c(Functioning.Day) )
# We also drop the column date
XY <- subset(XY, select = -c(Date, Time) ) # !!!!!! TIME

dim(XY)
```


```{r}
# Convert the factor "Holiday" into binary encoding
XY$Holiday<-ifelse(XY$Holiday == "Holiday", 1, 0)
XY$Holiday <- factor(XY$Holiday, levels = c(0, 1))
```



```{r}
# Tranform the response variable in order to make it more akin to a normal distribution
XY$Rented.Bike.Count <- log(XY$Rented.Bike.Count)
hist(XY$Rented.Bike.Count, main = "Log of Rented Bikes count")
```



We gather the dummy variables from the categorial features

```{r}
#df <- dummy_cols(df, select_columns = c("Seasons"))
#df <- subset(df, select = -c(Seasons) )
#df <- dummy_cols(df, select_columns = c("Time"))
#setnames(df,
#         old = c("Seasons_Autumn", "Seasons_Spring", "Seasons_Summer", "Seasons_Winter"),
#         new = c("Autumn", "Spring", "Summer", "Winter"))
#setnames(df,
#         old = c("Time_Afternoon", "Time_Evening", "Time_Morning", "Time_Night"),
#         new = c("Afternoon", "Evening", "Morning", "Night"))
```



```{r}
head(XY)
```


## 4. ANALYSIS

First we want to assess the extent of mutual relationships across numerical variables

Covariance matrix
```{r}
round(cov(XY[, 1:10]), 2)
```

Correlation Matrix

The major highlight here is the very high correlation ($0.91$) between *Temperature* and *Dew Point*
```{r}
XY.rcorr <- rcorr(as.matrix(XY[, 1:10]))
XY.corr.coeff <- XY.rcorr$r
XY.corr.p <- XY.rcorr$P
XY.corr.coeff
XY.corr.p
	
corrplot(cor(XY[, 1:10]))
```

```{r}
#Convert the actual dataframe into the proper type
XY <- as.data.frame(XY)
# Reset index
rownames(XY) <- 1:nrow(XY)
```


IMPORTANT ARTICLE: [here](https://medium.com/analytics-vidhya/how-to-proceed-from-simple-to-multiple-and-polynomial-regression-in-r-84b77f5673c5)

### 4.1. Multiple linear regression

```{r}
reg.init <- lm(Rented.Bike.Count~., data = XY)
summary(reg.init)
```

This first attempt shows how most of the regressors show value significantly different from zero, even if their estimates are most lower that $1e-1$ in absolute value.
Moreover, this model achieves a $R^2 = 0.6064$. Deeper analysis on this model is provided below.


#### 4.1.1. Diagnostic - Multiple Linear Regression

Check for normal distribution of the error terms.
```{r}
par(mfrow = c(1, 2))
plot(reg.init, 2)
plot(density(reg.init$residuals))
curve(dnorm(x, mean = mean(reg.init$residuals), sd = sd(reg.init$residuals)),
      add = T, col = 2)
```

From the plot of the residuals against the fitted values we can see the mean is very close to zero.
Moreover, apart from 3 outliers, the comparison between a normal distribution and the density estimation of the residuals shows a fair overlapping between the two. This result is confirmed also when looking at the QQ plot.
Finally, we can also show the absence of a prominent funnel shape in the residual plot, thus guiding us toward the assumption of constant variance of the error term.

```{r}
par(mfrow = c(2, 2))
plot(reg.init)
```

Variance Inflation Factor

To assess whether there is a presence of collinearity. From the correlation matrix shown in the previous part of the report, we could see that *Dew Point* and *Temperature* are highly correlated. This is reflected also when looking at the at the VIF. These two variables show the most extreme values in terms of VIF.
From now on we will consider only *Temperature*.

```{r}
vif(reg.init)
```

Analysis of variance

From the consideration of the ANOVA with respect to the factor variables, the procedures utilized are (1) Bartlett's test on equal variances (2) Fisher's test and (3) Turkey's method.
From the first we can already reject the null hypothesis of equal variances between different levels of the factor *Seasons*. Accordingly, we obtain the same result when it comes to the Fisher test.
Finally, in order to have a graphical representation of the impact of the difference levels of this factor variable on the target variables, we considered the Turkey's method, from which we can see the differences in means between groups.
```{r}
bartlett.test(XY$Rented.Bike.Count ~ XY$Seasons)
aov.seasons1 <- aov(XY$Rented.Bike.Count ~ XY$Seasons)
summary(aov.seasons1)
# Conditioning plot
coplot(XY$Rented.Bike.Count ~ XY$Temperature..C. | XY$Seasons)
turkey.seasons1 <- TukeyHSD(aov.seasons1)
turkey.seasons1

plot(turkey.seasons1)
```

```{r}
#####
# output of the first analysis:
# (1) remove outliers
XY <- XY[-c(3950, 4987, 6454),]
# (2) Remove Dew Point Temperature
XY <- subset(XY, select = -c(Dew.point.temperature..C.))
XY <- as.data.frame(XY)
dim(XY)
```


### 4.2. Multiple linear regression (2nd attempt)

This second attempt comes from the changes applied after the thoughts described above.

```{r}
reg.out2 <- lm(Rented.Bike.Count~., data = XY)
summary(reg.out2)
```

```{r}
par(mfrow = c(2, 2))
plot(reg.out2)
```

We now check whether most of the collinearity problems have been solved after removing *Dew Point Temperature*. Through the VIF we can see how this is true.

```{r}
vif(reg.out2)
```

From this second model, we don't see that much of an improvement in terms of the variance explained.


Thus, recalling the initial graphical analysis, we perform a further reconfiguration of the set of regressors, thus modelling the input space in a different way.
This of course will increase the bias of the model, as it comes from the following major assumption: **Regardless of how much it rains (or snows), the focus is on the event of raining (or snowing)**.
```{r}
snow.comparison <- data.frame( quantile(exp(XY[XY$Snowfall..cm. == 0,]$Rented.Bike.Count),
                                        seq = c(0, .25, .5, .75, 1)),
                               quantile(exp(XY[XY$Snowfall..cm. != 0,]$Rented.Bike.Count),
                                        seq = c(0, .25, .5, .75, 1)) )
names(snow.comparison) <- c("No snow", "Snow")
snow.comparison


rain.comparison <- data.frame( quantile(exp(XY[XY$Rainfall.mm. == 0,]$Rented.Bike.Count),
                                        seq = c(0, .25, .5, .75, 1)),
                               quantile(exp(XY[XY$Rainfall.mm. != 0,]$Rented.Bike.Count),
                                        seq = c(0, .25, .5, .75, 1)) )
names(rain.comparison) <- c("No rain", "Rain")
rain.comparison
```



```{r}
XY2 <- copy(XY)

XY2$Rainfall.mm. <- as.factor(ifelse(XY2$Rainfall.mm. == 0, 0, 1))
XY2$Snowfall..cm. <- as.factor(ifelse(XY2$Snowfall..cm. == 0, 0, 1))

XY2 <- as.data.frame(XY2)

reg.rain.snow.flag <- lm(Rented.Bike.Count~., data = XY2)
summary(reg.rain.snow.flag)
```


```{r}
par(mfrow = c(2, 2))
plot(reg.rain.snow.flag)
```

```{r}
vif(reg.rain.snow.flag)
```

The result shows definitely an improvement of the overall explained variance of the model. As a matter of fact, it attests to have the biggest adjusted $R^{2}$ so far, i.e. $0.6531$.

## 5. FURTHER STUDIES

The following part is aimed at deepen the study under two directories:

* Variable selection
* Regularization

```{r}
# Define a function that plots the results of a variable selection procedure
selection.result <- function(data) {
  
  par(mfrow = c(2, 2))
  
  # (1) Residual sum of squares
  plot(data$rss, xlab = "Number of Variables",
     ylab = "RSS", type = "l")
  
  # (2) Adjusted-R^2 with its largest value
  plot(data$adjr2, xlab = "Number of Variables", ylab = "Adjusted Rsq", type = "l")
  points(which.max(data$adjr2), data$adjr2[which.max(data$adjr2)],
         col = "red", cex = 2, pch = 20)
  
  # (3) Mallow's Cp with its smallest value
  plot(data$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
  points(which.min(data$cp), data$cp[which.min(data$cp)],
         col = "red", cex = 2, pch = 20)
  
  # (4) BIC with its smallest value
  plot(data$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
  
  points(which.min(data$bic), data$bic[which.min(data$bic)],
         col = "red",cex = 2, pch = 20)
  
  return(data$adjr2[which.max(data$adjr2)])
}
```


### 5.1. Variable selection - Best Subset
```{r}
reg.best.subset <- regsubsets(Rented.Bike.Count~., data = XY2,
                              nvmax = dim(XY)[2] - 1)
best.subset.summary <- summary(reg.best.subset)

selection.result(best.subset.summary)

par(mfrow = c(1, 1))
plot(reg.best.subset, scale = "adjr2")
```

### 5.2. Variable selection - Forward Selection
```{r}
reg.fwd.select <- regsubsets(Rented.Bike.Count~., data = XY2,
                             nvmax = dim(XY)[2] - 1, method = "forward")
fwd.select.summary <- summary(reg.fwd.select)

selection.result(fwd.select.summary)


par(mfrow = c(1, 1))
plot(reg.fwd.select, scale = "adjr2")
```

### 5.3. Variable selection - Backward Selection
```{r}
reg.bkw.select <- regsubsets(Rented.Bike.Count~., data = XY2,
                             nvmax = dim(XY)[2] - 1, method = "backward")
bkw.select.summary <- summary(reg.bkw.select)

selection.result(bkw.select.summary)

par(mfrow = c(1, 1))
plot(reg.bkw.select, scale = "adjr2")
```

So far, we have considered three different approaches for feature selection that either exploit an exhaustive search or greedy step-wise improvements.
As a matter of fact, *best subset selection* tries all the combination of variables per each model size (number of regressors), selects the best combination per size and yields the best size (with the corresponding best subset of predictors).
The other two, namely **backward stepwise** and **forward stepwise** perform greedy steps (under different metrics, e.g. Bayesian Information Criterion, Adjusted $R^{2}$, etc.) while removing (the former) or adding (the latter) one regressor at a time.

For the scope of the analysis we can see that the three methods yield the same result under each of the metrics considered (BIC, $Adj. R^{2}$, Mallow's C). This does not stop to the size of the subset of regressors, but also on the choice of the regressors themselves.

The equivalent behavior between forward and backward step-wise selection can be expected, but the same pattern shown by the best subset is an additional corroboration of the experimentation of variable selection.
Therefore, for this part of the report, we will set as a second baseline the model obtained with Best Subset Selection, under the $Adj. R^{2}$ metric, which is:

```{r}
best.subset.summary$which[which.max(best.subset.summary$adjr2),]
```


The above subset of regressors is further tried out using 10-fold cross validation. This shows again the the best subset yields a model with 8 predictors.

```{r}
set.seed(42)
folds <- sample(1:10, nrow(XY2), replace = TRUE)

cv.errors <- matrix(NA, 10, dim(XY2)[2] - 1)
colnames(cv.errors) <- 1:(dim(XY2)[2] - 1)


for(j in 1:10){
  best.fit <- regsubsets(Rented.Bike.Count~.,
                         data = XY2[folds!=j,], nvmax = (dim(XY2)[2] - 1))
  test.mat <- model.matrix(Rented.Bike.Count~., data = XY2[folds == j,])
  for(i in 1: (dim(XY2)[2] - 1)){
    coefi <- coef(best.fit, id=i)
    pred <- test.mat[, names(coefi)]%*%coefi
    cv.errors[j,i] <- mean( (XY2$Rented.Bike.Count[folds == j] - pred)^2)
  }
}

mean.cv.errors <- apply(cv.errors, 2, mean) 

plot(mean.cv.errors, type = "b",
     main = "Avg 10-fold CV errors",
     xlab = "Number of regressors")

points(which.min(mean.cv.errors),
       mean.cv.errors[which.min(mean.cv.errors)],
       col = "red", cex = 2, pch = 20)
```

```{r}
reg.best.subset.out <- lm(Rented.Bike.Count~.-Visibility..10m., data = XY2)
summary(reg.best.subset.out)
# Fit the best model and do the anova test (look at article for reference)
anova(reg.best.subset.out, reg.rain.snow.flag)
```

From the above ANOVA test, in which the null hypothesis can be formulated as: *The improvement in Adjusted R-squared is not statistically significant*.
We can notice how $p=0.6727$, thus much larger than $0.05$ associated to a 95% confidence. This means we don't reject the null hypothesis: in other words, by removing one variable we do not obtain a statistically significant improvement in the variance explained by the new model.

### 5.4. Train-test split
In the following part of the report we will analyze two shrinkage techniques:

* LASSO (also known as L1 penalty)
* Ridge (also known as L2 penalty or Weight Decay)

To provide a more reliable assessment of the performance of these regularization strategies, the implementation will again stick to the cross-validation approach.

```{r}
# design matrix
X.matrix <- model.matrix(Rented.Bike.Count~., XY2)

# remove the first column relative to the intercept
X.matrix <- X.matrix[,-1]

# vector of responses
y.vector <- XY2$Rented.Bike.Count

# SPLIT -----
train <- sample(1:nrow(XY2), nrow(XY2)*0.75)
test <- (-train)

X.train <- X.matrix[train, ]
X.test <- X.matrix[test, ]

y.train <- y.vector[train]
y.test <- y.vector[test]

print(dim(X.train))
print(dim(X.test))
```


```{r}
adjr2.function <- function(X.ref, v.pred, v.true){
  
  # Parameters
  # ----------
  # X.ref  : data matrix of the regressors
  # v.pred : y.hat (array of values predicted by the model)
  # v.true : ground truth array of values
  
  rss <- sum((v.true - v.pred)^2)
  tss <- sum((v.true - mean(v.true))^2)

  r2 <- (tss - rss)/tss
  
  adjr2 <- 1 - (1 - r2)*( (length(v.true) - 1)/(length(v.true) - dim(X.ref)[2] - 1) )
    
  output <- t(matrix(c(round(adjr2, 4),
                       round(r2, 4)) ) )
  output <- as.data.frame(output)
  colnames(output) <- c("Adj.R2", "R2")
  rownames(output) <- c("VALUE")
  return( output )
  }
```

### 5.5. Regularization tecnhiques: LASSO

```{r}
# LASSO with CV
lambda.lasso <- seq(0, 10, length = 200)


set.seed(42)
# alpha = 1: LASSO
lasso.cv <- cv.glmnet(X.train, y.train, alpha = 1,
                      nfold = 10, lambda = lambda.lasso)

plot(lasso.cv$lambda, lasso.cv$cvm, type = "l",
     main = "Training MSE of 10-fold CV Ridge",
     xlab = expression(lambda), ylab = "",
     ylim = c(0, max(lasso.cv$cvm)*1.1))

# To ensure a minimum extent of regularization, best lambda
# is chosen through the max function
best.lambda.lasso <- max(0.1, rev(lasso.cv$lambda)[2])
best.lambda.lasso
```

```{r}
lasso.pred.test <- predict(glmnet(X.train, y.train, alpha = 1,
                                  lambda = best.lambda.lasso),
                      s = best.lambda.lasso,
                      newx = X.test)

mean((lasso.pred.test - y.test)^2)

# Fit on the whole set of data
final.lasso <- glmnet(X.matrix, y.vector, alpha = 1,
                      lambda = best.lambda.lasso)
final.lasso.pred <- predict(final.lasso, newx = X.matrix,
                            s = best.lambda.lasso)

mean((final.lasso.pred - y.vector)^2)

# Compute the R2 and the Adjusted R2
noquote(c("R2: ",
          adjr2.function(X.matrix, final.lasso.pred, y.vector)$R2,
          "Adj. R2: ",
          adjr2.function(X.matrix, final.lasso.pred , y.vector)$Adj.R2) )
```



[Paper on feature selection](https://www.stat.cmu.edu/~ryantibs/papers/bestsubset.pdf)


### 5.6. Regularization tecnhiques: Ridge

```{r}
# Ridge with CV
lambda.ridge <- seq(0, 10, length = 200)


set.seed(42)
# alpha = 0: Ridge
ridge.cv <- cv.glmnet(X.train, y.train, alpha = 0,
                      nfold = 10, lambda = lambda.ridge)

plot(ridge.cv$lambda, ridge.cv$cvm, type = "l",
     main = "Training MSE of 10-fold CV Ridge",
     xlab = expression(lambda), ylab = "",
     ylim = c(0, max(ridge.cv$cvm)*1.1))

# To ensure a minimum extent of regularization, best lambda
# is chosen through the max function
best.lambda.ridge <- max(0.1, ridge.cv$lambda.1se)
best.lambda.ridge
```

We can see how as the intensity of the Ridge shrinkage method ($lambda$) increases, the standard error increases as well. This behavior is expected as we are adding more bias into the model, which in turn is not overfitting at all.
However, it is clear to see the effect of the regularization method from the effect that it has on the test error by looking at the difference between the two $MSE$ shown below.


```{r}
ridge.pred.test <- predict(glmnet(X.train, y.train, alpha = 0,
                                  lambda = best.lambda.ridge),
                      s = best.lambda.ridge,
                      newx = X.test)

mean((ridge.pred.test - y.test)^2)

# Fit on the whole set of data
final.ridge <- glmnet(X.matrix, y.vector, alpha = 0,
                      lambda = ridge.cv$lambda.1se)
final.ridge.pred <- predict(final.ridge, newx = X.matrix,
                            s = best.lambda.ridge)

mean((final.ridge.pred - y.vector)^2)

# Compute the R2 and the Adjusted R2
noquote(c("R2: ",
          adjr2.function(X.matrix, final.ridge.pred, y.vector)$R2,
          "Adj. R2: ",
          adjr2.function(X.matrix, final.ridge.pred, y.vector)$Adj.R2) )
```


Since both LASSO and RIDGE regressions introduce bias in the coefficients by design, statistical tests becomes inappropriate. Thus we will rely on the $Adj. R^{2}$ to assess whether to keep going with the current model or not.

Moreover, we can notice how the RIDGE penalty outperforms LASSO under the above mentioned metric. This behavior is expected as the "aggressiveness" of the LASSO regression in higher (the penalty is scales with the sign of the coefficients, rather than linearly with the euclidean norm, as it happens for the L2).
This difference can be shown when considering how the number of non zero components varies with respect to the intensity of the regularization (the comparison is eased since for both, as ${\lambda}$ increases the regularization "intensity" increases). As a matter of facts, we see how the components never get to zero for the considered grid of parameters with a L2 norm, while for the L1 penalty, all parameters get to zero very fast.


```{r}
plot(lasso.cv$lambda, lasso.cv$nzero, type = "b", col = "green",
     xlab = expression(lambda), ylab = "# of non-zero components",
     main = "Number of non-zero components against regularization intensity")
lines(ridge.cv$lambda, ridge.cv$nzero, type = "b", col = "blue")
legend(25, 7, c("RIDGE", "LASSO"), col = c("blue", "green"),
       lty = 1, cex = 0.8)
```


## 6. Model Selection

BEST MODELLO: best subset selection (stesso R^2 ma con parametri in meno)


- SE coefficients
- VIF
- Check residuals
- predicted output
- exp predicted output


## 7. Conclusion


- regularization not useful, anzi the opposite
- recall trattamento dati (log target, collinearity, etc.)




